{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxYU2phiPrqQ"
   },
   "source": [
    "**LSTM MODEL**\n",
    "1. Install and import dependencies\n",
    "2. Keypoints using MP Holistic\n",
    "3. Extract Keypoints\n",
    "4. Setup Folders for Collection\n",
    "5. Collect Keypoints Values for Training and Testing\n",
    "6. Preprocess Data and Create Labels and Features\n",
    "7. Build and Train LSTM Neural Network\n",
    "8. Make Predictions\n",
    "9. Save Weights\n",
    "10. Evaluation using Confusion Matrix and Accuracy\n",
    "11. Test in Real Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2vsTy0iSs4n"
   },
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3704,
     "status": "ok",
     "timestamp": 1656251963057,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "kOgzBCm3SUIr",
    "outputId": "308f88f6-fdff-4881-bfeb-28e45b711b60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: mediapipe in ./venv/lib/python3.10/site-packages (0.10.21)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.10/site-packages (3.10.7)\n",
      "Collecting tensorflowjs\n",
      "  Using cached tensorflowjs-4.22.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.10/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.10/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venv/lib/python3.10/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./venv/lib/python3.10/site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: jax in ./venv/lib/python3.10/site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in ./venv/lib/python3.10/site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.10/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./venv/lib/python3.10/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.10/site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting flax>=0.7.2 (from tensorflowjs)\n",
      "  Using cached flax-0.10.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib_resources>=5.9.0 (from tensorflowjs)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
      "  Using cached tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting tensorflow-hub>=0.16.1 (from tensorflowjs)\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting packaging (from tensorflow)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting msgpack (from flax>=0.7.2->tensorflowjs)\n",
      "  Using cached msgpack-1.1.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting optax (from flax>=0.7.2->tensorflowjs)\n",
      "  Using cached optax-0.2.6-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting orbax-checkpoint (from flax>=0.7.2->tensorflowjs)\n",
      "  Using cached orbax_checkpoint-0.11.30-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting tensorstore (from flax>=0.7.2->tensorflowjs)\n",
      "  Using cached tensorstore-0.1.78-cp310-cp310-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: rich>=11.1 in ./venv/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (14.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./venv/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (6.0.3)\n",
      "Collecting treescope>=0.1.7 (from flax>=0.7.2->tensorflowjs)\n",
      "  Using cached treescope-0.1.10-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./venv/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Collecting pandas (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Using cached pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Using cached wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ydf>=0.11.0 (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Using cached ydf-0.13.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.6 kB)\n",
      "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "INFO: pip is looking at multiple versions of ydf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting ydf>=0.11.0 (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Using cached ydf-0.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.6 kB)\n",
      "  Using cached ydf-0.11.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.11.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.10.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.17.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.9.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.8.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-macos==2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_macos-2.15.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-macos to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.8.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.0 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tensorflow_decision_forests-1.6.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.14.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-macos==2.14.1 (from tensorflow)\n",
      "  Using cached tensorflow_macos-2.14.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.14.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-macos==2.14.0 (from tensorflow)\n",
      "  Using cached tensorflow_macos-2.14.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of ydf to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached keras-3.11.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow-macos to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached keras-3.11.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "INFO: pip is still looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached keras-3.9.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "  Using cached keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.6.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Using cached jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.6.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting flax>=0.7.2 (from tensorflowjs)\n",
      "  Using cached flax-0.10.6-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.10.5-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.10.4-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.10.3-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.10.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached flax-0.8.5-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.8.4-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.8.3-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.8.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.8.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.7.5-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.7.4-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached flax-0.7.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-2.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting treescope>=0.1.7 (from flax>=0.7.2->tensorflowjs)\n",
      "  Using cached treescope-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_decision_forests-1.5.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting tensorflow-macos~=2.13.0 (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Using cached tensorflow_macos-2.13.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorflow-macos~=2.13.0 (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Using cached tensorflow_macos-2.13.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-2.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting markupsafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "  Using cached cffi-1.17.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "  Using cached cffi-1.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "  Using cached cffi-1.15.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.1 kB)\n",
      "  Using cached cffi-1.15.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "  Using cached cffi-1.14.6.tar.gz (475 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached cffi-1.14.5.tar.gz (475 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.3-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.6 kB)\n",
      "  Using cached sounddevice-0.5.2-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.6 kB)\n",
      "  Using cached sounddevice-0.5.1-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "  Using cached sounddevice-0.5.0-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "  Using cached sounddevice-0.4.7-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "  Using cached sounddevice-0.4.6-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "  Using cached sounddevice-0.4.5-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "  Using cached sounddevice-0.4.4-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.3 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Using cached setuptools-80.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Using cached setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "  Using cached setuptools-80.3.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "  Using cached setuptools-80.2.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "  Using cached setuptools-80.1.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "  Using cached scipy-1.15.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "  Using cached scipy-1.15.1-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "  Using cached scipy-1.15.0-cp310-cp310-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "  Using cached scipy-1.14.1-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.14.0-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.13.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.12.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Using cached mdurl-0.1.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Using cached mdurl-0.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax>=0.7.2->tensorflowjs)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1->flax>=0.7.2->tensorflowjs)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached pygments-2.19.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Using cached pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached pygments-2.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached pygments-2.17.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached Pygments-2.16.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mresolution-too-deep\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Dependency resolution exceeded maximum depth\n",
      "\u001b[31m╰─>\u001b[0m Pip cannot resolve the current dependencies as the dependency graph is too complex for pip to solve efficiently.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: Try adding lower bounds to constrain your dependencies, for example: \u001b[32m'package>=2.0.0'\u001b[0m instead of just \u001b[32m'package'\u001b[0m.\n",
      "\n",
      "Link: \u001b[4;94mhttps://pip.pypa.io/en/stable/topics/dependency-resolution/#handling-resolution-too-deep-errors\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1656251964679,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "zyk_Cj1lTBx4"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5mNukEBUexq"
   },
   "source": [
    "# Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1656252228703,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "acdAFfVgUl8Q"
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1656252305033,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "xjqvRhIgU3ED"
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR-CONVERSION BGR-to-RGB\n",
    "    image.flags.writeable = False                  # Convert image to not-writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Convert image to writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR-COVERSION RGB-to-BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1656252384608,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "dMNNayFJVPsc"
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1656252433029,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "DepCY5JWVgCF"
   },
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1656252693125,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "6vqu_ZooVqvF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765110459.882716  848026 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1765110459.961074  855809 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.977364  855804 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.981118  855806 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.982356  855804 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.983629  855805 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.989716  855805 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.997170  855803 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110459.997773  855804 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765110460.002061  855804 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "error",
     "timestamp": 1656252542997,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "YimPM9LgVwgV",
    "outputId": "0a48c792-cf94-4f5c-a40e-1025265bf220"
   },
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ty29e7QgLXa"
   },
   "source": [
    "# Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8y1FlSUhCwQ"
   },
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCUspdlZhc7f"
   },
   "source": [
    "# Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1656256035347,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "DyPx8mB0hgUs"
   },
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['cat', 'food', 'help'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1656256036925,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "4PcPkJnshi-8",
    "outputId": "d36e0106-54d6-4b8d-dbed-e8a1c7a1c647"
   },
   "outputs": [],
   "source": [
    "signs = ['cat','food','help']\n",
    "\n",
    "parent_folder = 'MP_Data'\n",
    "\n",
    "if not os.path.exists(parent_folder):\n",
    "    os.mkdir(parent_folder)\n",
    "    for sign in signs:\n",
    "        sign_folder = os.path.join(parent_folder, sign)\n",
    "        os.mkdir(sign_folder)\n",
    "        for i in range(30):\n",
    "            subfolder = os.path.join(sign_folder, str(i))\n",
    "            os.mkdir(subfolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH9h-pJ2hsgz"
   },
   "source": [
    "# Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 642,
     "status": "error",
     "timestamp": 1656255658196,
     "user": {
      "displayName": "Avishake Adhikary",
      "userId": "07253883199703046489"
     },
     "user_tz": -330
    },
    "id": "4m08aX8DhxTk",
    "outputId": "6a36cafb-2354-4bc9-8115-17355dab1479"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sommeZnbh8ki"
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_wqzJTIiB2p"
   },
   "source": [
    "# Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7gsasOUiFwV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pHKTsQ0iHq9"
   },
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSLsH2qgpqoH"
   },
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gE7Cgzhlptlo"
   },
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBN3Pl15p2wK"
   },
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUwO3RHWp7aC"
   },
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUzql9nSp9bp"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TboyXNdaqBXa"
   },
   "source": [
    "# Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftwthsRCqD83"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5by_FSSqF5z"
   },
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "#While training access tensorboard\n",
    "#tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pe-dANRGqHkD"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwaSGrThqJ36"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qE4K3bvqMLI"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDl33ZbFqN2t"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "visualkeras.layered_view(model, legend=True, font=font,spacing=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhYu2JVPqbp-"
   },
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tE8zfpYqeDe"
   },
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNRZlz79qiAA"
   },
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtZeBCPgqjz4"
   },
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPsjE16HqnKo"
   },
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld-yL7oFqpgg"
   },
   "outputs": [],
   "source": [
    "model.save('./model.h5')\n",
    "model.save_weights('./model_weights.h5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTL-aUUhqrKK"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qphZ5ktqsMe"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8daBYvRkqvGx"
   },
   "source": [
    "# Evaluation using Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPcHFXpsqyaX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iVft9Y3qz61"
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNBgddNGq1le"
   },
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgLBnVFnq3iJ"
   },
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tphmefeRq4Qk"
   },
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoPKuYiFq9eh"
   },
   "source": [
    "# Realtime Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6BrEyu7rADM"
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29FjbYT-rBc-"
   },
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZF5_CJWrU0a"
   },
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "signs = ['hello', 'thanks', 'iloveyou']\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR-CONVERSION BGR-to-RGB\n",
    "    image.flags.writeable = False                  # Convert image to not-writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Convert image to writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR-COVERSION RGB-to-BGR\n",
    "    return image, results\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "    \n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "model.load_weights('model_weights.h5')\n",
    "model.summary()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            #print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            #image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        #print(sentence)\n",
    "        #word = sentence[-1:]\n",
    "        #convert_to_audio(word[0]) if (len(word) != 0) else print(\"word not detected yet\")\n",
    "        #convert_to_audio(word[0])\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('Realtime LSTM Sign Language Detection', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMm951zd4hae6QExkf4AOcW",
   "collapsed_sections": [
    "B2vsTy0iSs4n",
    "S5mNukEBUexq",
    "1Ty29e7QgLXa",
    "TboyXNdaqBXa"
   ],
   "name": "RealTimeSignLanguageDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
